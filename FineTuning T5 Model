{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5241527,"sourceType":"datasetVersion","datasetId":3049717}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importing the Required Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nimport torch\nfrom torch.nn import *\nfrom torch.utils.data import Dataset,DataLoader\n\nimport transformers\nfrom transformers import T5ForConditionalGeneration,T5Tokenizer,T5Config\n\n!pip install lightning \nimport lightning as L\nimport warnings\nwarnings.filterwarnings(action='ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading the Dataset for the Task**","metadata":{}},{"cell_type":"code","source":"text_dataset=pd.read_csv(r'/kaggle/input/managpt-4080-nlp-prompts-and-generated-texts/ManaGPT-1020_4080_prompts_and_generated_texts.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#text_dataset.drop_duplicates(subset='generated_response',keep='first',inplace=True)\n#text_dataset.reset_index(inplace=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_dataset.columns\ntext_dataset.drop(columns=['subject_of_prompt', 'modal_variant_of_prompt','generated_response_excluding_prompt'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SPLIT=40\ntrain_data=text_dataset[:int((SPLIT/100)*len(text_dataset))]\nvalidation_data=text_dataset[int((SPLIT/100)*len(text_dataset)):int(((SPLIT+20)/100)*len(text_dataset))]\ntest_data=text_dataset[int(((SPLIT+20)/100)*len(text_dataset)):]\n\nprint(train_data.shape)\nprint(validation_data.shape)\nprint(test_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading the Pretrained Goggle-T5 Model**","metadata":{}},{"cell_type":"code","source":"\nMODEL_NAME='t5-base'\nmodel=T5ForConditionalGeneration.from_pretrained(MODEL_NAME)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading the Pretrained Goggle- T5 Tokenizer**","metadata":{}},{"cell_type":"code","source":"#!pip install sentencepiece\ntokenizer=T5Tokenizer.from_pretrained(MODEL_NAME)\ntokenizer.special_tokens_map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"special_tokens_dict = {'additional_special_tokens': ['[EOS]','[SOS]']}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Creating a Data Generator in Pytorch for Distributed Computing**","metadata":{}},{"cell_type":"code","source":"class DataGenerator(Dataset):\n\n    def __init__(self, dataset=None,tokenizer=None):\n\n        self.dataset_= dataset\n        self.tokenizer=tokenizer\n\n\n    def __len__(self):\n       \n        return self.dataset_.shape[0]\n\n    def add_special_tokens(self, tokens):\n        text_tokens=tokens\n        B,S = text_tokens.size()\n        text_tokens_=torch.zeros(B,S+2)\n        text_tokens_[:B,1:S+1]=text_tokens\n        text_tokens_[:B,0]=tokenizer.convert_tokens_to_ids('[SOS]')\n        text_tokens_[:B,-1]=tokenizer.convert_tokens_to_ids('[EOS]')\n        return text_tokens_\n\n    def GlobalPadding(self,tokens,max_length):\n\n        B,S = tokens.size()\n\n        tokens_=torch.zeros(B,max_length-2)\n        \n\n        tokens_[:B,:S]=tokens\n\n\n        return tokens_\n\n    def __getitem__(self, idx):\n        \n        max_length=300\n        \n        assert type(idx)==int, f'Expected {integer} got type {idx}'\n\n        data= self.dataset_.iloc[idx,:]\n\n        prompt,generated_text = data['complete_prompt'],data['generated_response']\n    \n        \n        input_text_tokens = tokenizer(prompt, padding=True, truncation=True, return_tensors=\"pt\",add_special_tokens=False,max_length=max_length)\n\n        output_text_tokens = tokenizer(str(generated_text), padding=True, truncation=True, return_tensors=\"pt\",max_length=max_length,add_special_tokens=False)\n\n        \n        input_text_tokens['input_ids']= self.add_special_tokens(self.GlobalPadding(input_text_tokens['input_ids'],max_length=max_length)).int()\n        \n        output_text_tokens['input_ids']= self.add_special_tokens(self.GlobalPadding(output_text_tokens['input_ids'],max_length=max_length)).int()\n\n        BI,SI = input_text_tokens['input_ids'].size()\n\n        B,S=output_text_tokens['input_ids'].size()\n\n        output_text_tokens['input_ids']=output_text_tokens['input_ids'][:,:S].view(B,S)\n\n        B,S=output_text_tokens['input_ids'].size()\n        \n        B,S=output_text_tokens['input_ids'].view(B,S).size()\n        \n        \n        input_tokens=input_text_tokens['input_ids'].view(BI,SI)\n        attention_mask=torch.zeros_like(input_tokens)\n        attention_mask.masked_fill(input_tokens!=0,1)\n\n        sample={'input_ids':input_text_tokens['input_ids'].view(BI,SI),\n                'attention_mask':attention_mask,\n                'labels':output_text_tokens['input_ids'].view(B,S)}\n        \n        return sample\n        \n             \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Adapting the Custom Data Generator With Pytorch Lighting DataModule**","metadata":{}},{"cell_type":"code","source":"class T5Generator(L.LightningDataModule):\n    def __init__(self,train,test,valid,batch_size,tokenizer):\n        super().__init__()\n        self.train_data=train\n        self.test_data=test\n        self.validation_data=valid\n        self.batch_size=batch_size\n        self.tokenizer=tokenizer\n    def setup(self,stage=None):\n        pass\n    def prepare_data(self):\n        \n        self.train_data=DataGenerator(dataset=self.train_data,tokenizer=self.tokenizer)\n        self.test_data=DataGenerator(dataset=self.test_data,tokenizer=self.tokenizer)\n        self.validation_data=DataGenerator(dataset=self.validation_data,tokenizer=self.tokenizer)\n                    \n    def train_dataloader(self):\n     \n        return DataLoader(self.train_data,batch_size=5,shuffle=True,num_workers=2)\n    \n    def test_dataloader(self):\n        \n        return DataLoader(self.test_data,batch_size=5,shuffle=True,num_workers=2)\n    \n    def val_dataloader(self):\n        \n        return DataLoader(self.validation_data,batch_size=5,shuffle=True,num_workers=2)\n\ndata_module=T5Generator(train=train_data,\n            test=test_data,\n            valid=validation_data,\n            tokenizer=tokenizer,\n            batch_size=2)\n\ndata_module.setup()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Fusing th Loaded T5- Model with pytorch Lightning for distributed Training and Inference**","metadata":{}},{"cell_type":"code","source":"devic='cuda' if torch.cuda.is_available() else 'cpu'\nclass GPT(L.LightningModule):\n\n    def __init__(self,model):\n        super().__init__()\n        self.model=model\n        self.outputs=None\n        #self.loss_fn=CrossEntropyLoss\n        self.loss=None\n    \n    \n    def forward(self, input_ids,attention_mask, labels=None):\n        device='cuda' if torch.cuda.is_available() else 'cpu'\n\n        \n        B,_,S=input_ids.shape\n        \n        input_ids=input_ids.view(B,S).to(device)\n        \n        attention_mask=attention_mask.view(B,S).to(device)\n\n        BT,_,ST=labels.shape\n        \n        labels=labels.view(BT,ST).type(torch.LongTensor).to(device)\n        \n        #print(input_ids.shape,attention_mask.shape,labels.shape)\n        \n        self.outputs=self.model(\n                                input_ids=input_ids,\n                                attention_mask=attention_mask,\n                                labels=labels\n                                )\n        self.loss=self.outputs.loss\n        \n        \n        return self.loss\n    \n    def training_step(self,batch,batch_idx):\n        \n        input_ids=batch['input_ids']\n        \n        attention_mask=batch['attention_mask']\n        \n        labels=batch['labels']\n        \n        train_loss=self.forward(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n        \n        self.log('Train_Loss',train_loss,prog_bar=True,logger=True)\n    \n    \n    def validation_step(self,batch,batch_idx):\n        \n        input_ids=batch['input_ids']\n        \n        attention_mask=batch['attention_mask']\n        \n        labels=batch['labels']\n        \n        val_loss=self.forward(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n        \n        self.log('Val_Loss',val_loss,prog_bar=True,logger=True)\n        \n    def test_step(self,batch,batch_idx):\n        \n        input_ids=batch['input_ids']\n        \n        attention_mask=batch['attention_mask']\n        \n        labels=batch['labels']\n        \n        test_loss=self.forward(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n        \n        self.log('test_Loss',test_loss,prog_bar=True,logger=True)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n\n        return [optimizer], [lr_scheduler]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Initializing the Model** ","metadata":{}},{"cell_type":"code","source":"summarizer=GPT(model=model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Defining the Model Callbacks and Checkpoints**","metadata":{}},{"cell_type":"code","source":"from lightning.pytorch.callbacks import ModelCheckpoint\ncheckpoint_callback = ModelCheckpoint(\n    monitor='Val_Loss',\n    dirpath='/kaggle/working/GPT_CHEKPOINTS',\n    filename='GPT-{epoch:02d}-{val_loss:.2f}',\n    save_last=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Defining the Trainer Method of the Lightning Module**","metadata":{}},{"cell_type":"code","source":"trainer=L.Trainer(\n    max_epochs=100,\n    enable_progress_bar=True,\n    num_sanity_val_steps=0,\n    callbacks=[checkpoint_callback]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Activating the tensor board for Logging various Evaluation Metrics**","metadata":{}},{"cell_type":"code","source":"!pip install tensorboard","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text=\"summarize: In a quaint little town where the cobblestone streets echoed  the melodies of daily life, there lived a clockmaker named Mr. Wren. His shop was filled with timepieces of every kind, but his pride was a delicate, mechanical sparrow that could sing.One day, a curious child named Lila entered the shop. She was mesmerized by the sparrow and asked, 'Does it feel the wind when it sings?' Mr. Wren, with a twinkle in his eye, replied, 'Why don't we find out?'As the sun set, the sparrow returned to its perch, and Lila learned that sometimes, the line between the crafted and the real is as thin as a sparrow's feather.And from that day on, the clockmaker's shop became a place where not just clocks but dreams were made, reminding everyone that magic can be found in the most unexpected places.\"\nsample_encodings=tokenizer(sample_text,\n                            return_tensors='pt',\n                            max_length=218).input_ids\ngenerated_ids=model.generate(sample_encodings)\npredicted_tokens=tokenizer.batch_decode(generated_ids, remove_special_tokens=True,clean_up_tokenization_spaces=True)\n\nprint('Prompt :')\nprint('\\n'+ sample_text)\nprint()\nprint('\\n'+ 'Model Response :')\nprint('\\n'+ f\"{' '.join(predicted_tokens)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%tensorboard --logdir ./lightning_logs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **FineTunning the T5 Model for Summarization Task**","metadata":{}},{"cell_type":"code","source":"trainer.fit(summarizer,datamodule=data_module)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading the Model from Checkpoint after Fine Tunining**","metadata":{}},{"cell_type":"code","source":"!pip install  gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gdown \nurl = 'https://drive.google.com/file/d/1-7XOO0B1QIQavDFDsofoX5NB8UUKIZvi/view?usp=drive_link' \noutput = 'last.ckpt'\ngdown.download(url,output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path='/kaggle/working/last.ckpt'\n\nconfig = T5Config.from_pretrained(\"t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(model_path,config=config,from_tf=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.load(model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=GPT.load_from_checkpoint('/kaggle/working/last.ckpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}